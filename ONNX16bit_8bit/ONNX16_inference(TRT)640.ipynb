{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "Available Physical Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Available Logical Devices: [LogicalDevice(name='/device:CPU:0', device_type='CPU'), LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "GPU is available and recognized by TensorFlow.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass\n",
    "print(tf.__version__)\n",
    "# Print list of all available physical devices (including GPUs)\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(\"Available Physical Devices:\", physical_devices)\n",
    "\n",
    "# Print list of all available logical devices\n",
    "logical_devices = tf.config.list_logical_devices()\n",
    "print(\"Available Logical Devices:\", logical_devices)\n",
    "\n",
    "# Check if GPU is available\n",
    "gpu_available = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_available:\n",
    "    print(\"GPU is available and recognized by TensorFlow.\")\n",
    "else:\n",
    "    print(\"No GPU is available or TensorFlow doesn't recognize the GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "(50, 640, 640, 1)\n",
      "<dtype: 'float16'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import numpy as np\n",
    "import cv2,os,math,keras,torch,time\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input\n",
    "import tensorflow.keras.backend as K\n",
    "data_size = 960\n",
    "x_test = np.load(f'C:/Users/Unknown/Documents/VSCode/Diploma/Validation{data_size}_half.npy')\n",
    "output_size=640,640\n",
    "def imresize(array,shape):\n",
    "    resized_images = np.empty((array.shape[0], shape, shape), dtype=array.dtype)\n",
    "    for i in range(array.shape[0]):\n",
    "        # Extract the current image\n",
    "        current_image = array[i, :, :]  # Assuming the channel axis is the last one\n",
    "        # Resize the image using cv2.resize\n",
    "        resized_image = cv2.resize(current_image, (shape, shape), interpolation=cv2.INTER_LINEAR)\n",
    "        # Add the resized image to the new array\n",
    "        resized_images[i, :, :] = resized_image\n",
    "    return resized_images\n",
    "\n",
    "resize=1\n",
    "if resize:\n",
    "    shape=output_size[0]\n",
    "    x_test=imresize(x_test,shape)\n",
    "x_test=tf.image.convert_image_dtype(x_test,dtype='float16')#do not use bfloat16. it is only for TPU and AVX-512 for now.\n",
    "\n",
    "#plt.imshow(tf.cast(x_test[0],dtype='float32'))\n",
    "x_test = tf.expand_dims(x_test, axis=-1)\n",
    "#confirm they are idential except N\n",
    "print(x_test.shape)\n",
    "print(x_test.dtype)\n",
    "\n",
    "def PropagationLayer(input,mode=None):#Angular spectrum method for input or output\n",
    "    input=tf.cast(input,dtype='float32')#estimated POH itself\n",
    "    inputs=Input((output_size[0],output_size[1],1))\n",
    "    input=tf.squeeze(input, axis=-1)#to eliminate calculation error in ASM\n",
    "    lamda = 532e-9 #lambda is the reserved word\n",
    "    dp = 8e-6 #pixel size of SLM you use\n",
    "    z = 100e-3 #propagation distance you assume\n",
    "    pad_m = inputs.shape[1] // 2\n",
    "    pad_n = inputs.shape[2] // 2\n",
    "    if mode==\"input\":\n",
    "        padded_real = tf.math.cos(input)\n",
    "        padded_imaginary = tf.math.sin(input)\n",
    "        z = -z #propagation distance,bu reverse direction\n",
    "    else: #input is 0 to 1 data by sigmoid\n",
    "        padded_real = tf.math.cos(2*math.pi*input)\n",
    "        padded_imaginary = tf.math.sin(2*math.pi*input)\n",
    "    inputsc = tf.complex(padded_real, padded_imaginary)#complex amp of POH, same as exp(1j*...)\n",
    "    Lx = dp * inputs.shape[2]\n",
    "    Ly = dp * inputs.shape[1]\n",
    "    f_max = 0.5 / dp\n",
    "    du = 1.0 / Lx\n",
    "    dv = 1.0 / Ly\n",
    "    x_range = tf.range(-f_max, f_max, du, dtype=tf.float32)\n",
    "    y_range = tf.range(-f_max, f_max, dv, dtype=tf.float32)\n",
    "    fx, fy = tf.meshgrid(x_range, y_range)\n",
    "    FH = tf.signal.fftshift(tf.signal.ifft2d(tf.signal.fftshift(inputsc)))\n",
    "    real_part = tf.math.cos(2 * math.pi * z * tf.sqrt(1 /(lamda**2) - fx**2 - fy**2))\n",
    "    imaginary_part = tf.math.sin(2 * math.pi * z * tf.sqrt(1 /(lamda**2) - fx**2 - fy**2))\n",
    "    P = tf.complex(real_part, imaginary_part)#normal ASM kernel\n",
    "    #define frequency limit for sampling theorem\n",
    "    fx_max = tf.math.divide(inputs.shape[1] * dp, lamda * tf.math.sqrt((2 * z) ** 2 + (inputs.shape[1]  * dp) ** 2))\n",
    "    fy_max = tf.math.divide(inputs.shape[2] * dp, lamda * tf.math.sqrt((2 * z) ** 2 + (inputs.shape[2]  * dp) ** 2))\n",
    "    P = tf.where(tf.math.logical_and(tf.abs(fx) < fx_max, tf.abs(fy) < fy_max), x=P, y=0.0)#band-limiting mask for kernal(sampling theorem)\n",
    "\n",
    "    P = tf.expand_dims(P, axis=0)  # Add a batch dimension\n",
    "    U = tf.signal.fftshift(tf.signal.fft2d(tf.signal.fftshift(FH * P)))#kernel applied in Fourier domain\n",
    "    if mode==\"input\":\n",
    "        U = tf.expand_dims(U, axis=-1)\n",
    "        asm = tf.concat([tf.math.real(U), tf.math.imag(U)], axis=-1)#complex amp\n",
    "    else:\n",
    "        asm = tf.square(tf.abs(U)) #observed info\n",
    "    return asm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorRT Engine creation takes some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "import onnx,onnxruntime\n",
    "model = onnx.load(\"./Unet_POH_640fixed_16bit.onnx\")\n",
    "#!onnxsim \"./Unet_POH_512.onnx\" \"./Unet_POH_512_sim.onnx\"\n",
    "#onnx.checker.check_model(model)\n",
    "#print(onnx.helper.printable_graph(model.graph))\n",
    "options = onnxruntime.SessionOptions()\n",
    "#options.enable_profiling=True\n",
    "print(onnxruntime.get_available_providers())#list available ep\n",
    "os.environ[\"ORT_TENSORRT_FP16_ENABLE\"] = \"1\"  # Enable FP16 precision\n",
    "#os.environ[\"ORT_TENSORRT_INT8_ENABLE\"] = \"1\"  # Enable INT8 precision\n",
    "os.environ[\"ORT_TENSORRT_ENGINE_CACHE_ENABLE\"] = \"1\"  # Enable engine caching\n",
    "ort_session = onnxruntime.InferenceSession('./Unet_POH_640fixed_16bit.onnx',sess_options=options,providers=['TensorrtExecutionProvider'])\n",
    "input_name = model.graph.input[0].name #usually it is input_1\n",
    "output_names = [output.name for output in model.graph.output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 640, 640, 1)\n",
      "Inference + Data prep time for each image: 0.01660282611846924 seconds\n",
      "Inference time for each image: 0.013902831077575683 seconds\n",
      "total inference:0.16602826118469238\n",
      "(10, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "input = x_test[21:31]\n",
    "print(input.shape)\n",
    "if input.shape[0] == x_test.shape[1]:#single image case\n",
    "    data_num=1\n",
    "else:\n",
    "    data_num=input.shape[0]\n",
    "#torch.cuda.synchronize() #it is for accurate time measurement with GPU, but I chose profile feature of onnxruntime\n",
    "start_time = time.time()\n",
    "input_datas = (tf.cast(PropagationLayer(input,mode=\"input\"),dtype='float16')).numpy()\n",
    "start_time2 = time.time()\n",
    "holo = ort_session.run(output_names, {input_name: input_datas})\n",
    "\n",
    "#torch.cuda.synchronize()\n",
    "end_time = time.time()\n",
    "inference_time = (end_time - start_time)/(data_num)\n",
    "inference_time2 = (end_time - start_time2)/(data_num)\n",
    "print(f\"Inference + Data prep time for each image: {inference_time} seconds\")#with data prep\n",
    "print(f\"Inference time for each image: {inference_time2} seconds\")#inference only\n",
    "print(f\"total inference:{(end_time - start_time)}\")\n",
    "holo=holo[0]\n",
    "output=PropagationLayer(holo,mode=\"output\")\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session.end_profiling()#stop onnx profiling\n",
    "n = 11\n",
    "plt.figure(figsize=(n+2, 4), dpi=250)\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "input=tf.cast(input,dtype='float32')\n",
    "for i in range(1, n):#1 to n-1\n",
    "    # Display original\n",
    "    ax = plt.subplot(3, n, i)\n",
    "    plt.imshow(input[i-1])\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if i==1:\n",
    "        ax.set_title(\"Original\")  # Set a title for this subplot\n",
    "    # Display reconstructed image\n",
    "    ax = plt.subplot(3, n, i + n)\n",
    "    plt.imshow(output[i-1])  # Reconstructed one\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if i==1:\n",
    "        ax.set_title(\"Reconstructed\")  # Set a title for this subplot\n",
    "    # Display hologram\n",
    "    ax = plt.subplot(3, n, i + 2 * n)\n",
    "    plt.imshow(holo[i-1])  # Reconstructed one\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if i==1:\n",
    "        ax.set_title(\"Hologram\")  # Set a title for this subplot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "num=25\n",
    "plt.imshow(output[num-21],cmap='gray')\n",
    "plt.figure(figsize=(6, 5),dpi=250)\n",
    "# Subplot 1: Reconstructed hologram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(output[num-21],cmap='gray')# tf.math.log(tf.abs(D)) for good visualization\n",
    "plt.title('Reconstructed hologram with DNN')\n",
    "plt.axis('image')\n",
    "# Subplot 2: Original image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(tf.cast(input[num-21],dtype=\"float32\"), cmap='gray')\n",
    "plt.title('Original image')\n",
    "plt.axis('image')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate single image with several criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from onnxruntime.quantization import QuantizationMode, quantize_dynamic,QuantType,quantize_static\\nquantized_model = quantize_dynamic(\"./Unet_POH_512.onnx\", \"./Unet_POH_512_quantized.onnx\", weight_type=QuantType.QUInt8)\\n\\nmodel = onnx.load(\"./Unet_POH_512_quantized.onnx\")\\n#!onnxsim \"./Unet_POH_512.onnx\" \"./Unet_POH_512_sim.onnx\"\\n#onnx.checker.check_model(model)\\n#print(onnx.helper.printable_graph(model.graph))\\noptions = onnxruntime.SessionOptions()\\n#options.enable_profiling=True\\nort_session = onnxruntime.InferenceSession(\\'./Unet_POH_512_quantized.onnx\\',sess_options=options,providers=[\\'DmlExecutionProvider\\'])\\n# Get the input name from the model\\ninput_name = model.graph.input[0].name\\noutput_names = [output.name for output in model.graph.output]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from onnxruntime.quantization import QuantizationMode, quantize_dynamic,QuantType,quantize_static\n",
    "quantized_model = quantize_dynamic(\"./Unet_POH_512.onnx\", \"./Unet_POH_512_quantized.onnx\", weight_type=QuantType.QUInt8)\n",
    "\n",
    "model = onnx.load(\"./Unet_POH_512_quantized.onnx\")\n",
    "#!onnxsim \"./Unet_POH_512.onnx\" \"./Unet_POH_512_sim.onnx\"\n",
    "#onnx.checker.check_model(model)\n",
    "#print(onnx.helper.printable_graph(model.graph))\n",
    "options = onnxruntime.SessionOptions()\n",
    "#options.enable_profiling=True\n",
    "ort_session = onnxruntime.InferenceSession('./Unet_POH_512_quantized.onnx',sess_options=options,providers=['DmlExecutionProvider'])\n",
    "# Get the input name from the model\n",
    "input_name = model.graph.input[0].name\n",
    "output_names = [output.name for output in model.graph.output]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all_holo = []\\n\\ninput = x_test[20:30]\\nprint(input.shape)\\n# Run the model\\nstart_time = time.time()\\ninput_datas = (PropagationLayer(input,mode=\"input\"))\\nstart_time2 = time.time()\\nfor input_data in input_datas:\\n    input_data = np.expand_dims(input_data, axis=0)\\n    holo = ort_session.run(output_names, {input_name: input_data})\\n    all_holo.append(holo)\\nend_time = time.time()\\ninference_time = (end_time - start_time)/(input.shape[0])\\ninference_time2 = (end_time - start_time2)/(input.shape[0])\\nprint(f\"Inference time for each image: {inference_time} seconds\")#with data prep\\nprint(f\"Prediction time for each image: {inference_time2} seconds\")\\nholo = np.squeeze(all_holo)\\nholo= tf.expand_dims(holo, axis=-1)\\noutput=PropagationLayer(holo,mode=\"output\")\\nprint(output.shape)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''all_holo = []\n",
    "\n",
    "input = x_test[20:30]\n",
    "print(input.shape)\n",
    "# Run the model\n",
    "start_time = time.time()\n",
    "input_datas = (PropagationLayer(input,mode=\"input\"))\n",
    "start_time2 = time.time()\n",
    "for input_data in input_datas:\n",
    "    input_data = np.expand_dims(input_data, axis=0)\n",
    "    holo = ort_session.run(output_names, {input_name: input_data})\n",
    "    all_holo.append(holo)\n",
    "end_time = time.time()\n",
    "inference_time = (end_time - start_time)/(input.shape[0])\n",
    "inference_time2 = (end_time - start_time2)/(input.shape[0])\n",
    "print(f\"Inference time for each image: {inference_time} seconds\")#with data prep\n",
    "print(f\"Prediction time for each image: {inference_time2} seconds\")\n",
    "holo = np.squeeze(all_holo)\n",
    "holo= tf.expand_dims(holo, axis=-1)\n",
    "output=PropagationLayer(holo,mode=\"output\")\n",
    "print(output.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''all_holo = []\n",
    "input = x_test[21:31]\n",
    "print(input.shape)\n",
    "if input.shape[0] == x_test.shape[1]:#single image case\n",
    "    data_num=1\n",
    "else:\n",
    "    data_num=input.shape[0]\n",
    "#torch.cuda.synchronize() #it is for accurate time measurement with GPU, but I chose profile feature of onnxruntime\n",
    "start_time = time.time()\n",
    "input_datas = (tf.cast(PropagationLayer(input,mode=\"input\"),dtype='float16'))\n",
    "start_time2 = time.time()\n",
    "for input_data in input_datas:\n",
    "    input_data = np.expand_dims(input_data, axis=0)\n",
    "    holo = ort_session.run(output_names, {input_name: input_data})\n",
    "    all_holo.append(holo)\n",
    "#torch.cuda.synchronize()\n",
    "end_time = time.time()\n",
    "inference_time = (end_time - start_time)/(data_num)\n",
    "inference_time2 = (end_time - start_time2)/(data_num)\n",
    "print(f\"Inference + Data prep time for each image: {inference_time} seconds\")#with data prep\n",
    "print(f\"Inference time for each image: {inference_time2} seconds\")#inference only\n",
    "print(f\"total inference:{(end_time - start_time)}\")\n",
    "holo = np.squeeze(all_holo)\n",
    "holo= tf.expand_dims(holo, axis=-1)\n",
    "output=PropagationLayer(holo,mode=\"output\")\n",
    "print(output.shape)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
